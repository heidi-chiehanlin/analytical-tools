{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNIVARIATE ANALYSIS\n",
    "\n",
    "**Part 1. Filters** \n",
    "\n",
    "Feature analysis for regression models takes a lot of time and manual work, but not all the features are worth to do so.\n",
    "In this step, we can exclude up to 90% of the features, depends on the data.\n",
    "1. `find_informative_features`: Designed for sparse data, the function tells whether **being null or not** is a good indicator.\n",
    "2. `anova_analysis`: Rank features by the value of f-test and chi-squared test. Then keep features that distributes more differently in two groups (target=1 and target=0).\n",
    "\n",
    "**Part 2. Thresholds** (for numerical features)\n",
    "\n",
    "When the original distribution is not ideal for linear/non-linear transformation, we group data into bins that optimize the linear trend of event rates. The self-defined class we used here is called `Binner`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tools, Environment Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                \n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "# custom libraries\n",
    "from binner import NumericFeatureBinner, NominalFeatureBinner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv('....csv')\n",
    "print(\"data shape: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "\n",
    "    df = copy.deepcopy(data)\n",
    "    \n",
    "    # set target value name = 'y'\n",
    "    # set target value to binary \n",
    "    df['y'] = df['label']\n",
    "    df = df.replace({'y':{'a':0, 'b':1, 'c':1, 'd':1}})\n",
    "    \n",
    "    # drop unwanted columns\n",
    "    drop_key = ... # list of columns\n",
    "    df = df.drop(labels=drop_key, axis=1)\n",
    "\n",
    "    print('============== processed data info =============')\n",
    "    print(df.shape)\n",
    "                \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.preprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Informative\n",
    "\n",
    "The function finds out the features that are more informative in a sparse dataset. In each feature, assume null value = 0 and 1 otherwise. The porportion of events in which feature = 1 tells whether being null or not is a good indicator of event.\n",
    "\n",
    "For example:\n",
    "* Feature A has 80% null values, and let target = 0 or 1. \n",
    "* The function will take the rest 20% records, sum the target and divide by the count of datapoints. It's also called \"target rate\".\n",
    "* The funtion returns features that pass the given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_informative_features(data, target_rate_threshold):\n",
    "    stats = {}\n",
    "\n",
    "    # select features to be passed to this filter\n",
    "    f_list = data.columns.tolist()\n",
    "    f_list = ... # customize\n",
    "    \n",
    "    for f in f_list:\n",
    "        y = data['y']\n",
    "        notna_data = data[data.notna()]\n",
    "        notna_y = y[data.notna()]\n",
    "        \n",
    "        stats[f] = {\n",
    "            'feature': f,\n",
    "            'notna_count': notna_data.count(),\n",
    "            'notna_target': notna_y.sum(),\n",
    "            'notna_target_rate': round(notna_y.sum()/notna_data.count(),4)\n",
    "        }\n",
    "    \n",
    "    # turn stats into table\n",
    "    table = pd.DataFrame.from_dict(stats)\n",
    "    table = table.T.reset_index(drop=True)\n",
    "    \n",
    "    # keep those strong features\n",
    "    filtered = table[table.notna_target_rate >= target_rate_threshold].sort_values(by=['notna_target_rate','notna_target'], ascending=False)\n",
    "    filtered = pd.concat([filtered, keep])\n",
    "    filtered = filtered.reset_index(drop=True)\n",
    "    \n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "informative_table = find_informative_features(data, 0.05)\n",
    "informative_features = table[\"feature\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. ANOVA test\n",
    "\n",
    "The idea of ANOVA test is to examine whether the population of multiple independent samples are significantly different. \n",
    "\n",
    "Here, we seperate features into two sample group according to their `Y` (Y is binary). \n",
    "If the ANOVA test finds that the population of two sample groups are different, then we expect this feature to be a good indicator to seperate `Y=0` and `Y=1`.\n",
    "\n",
    "For different data types, we apply different test to measure the variance between two smaple's population.\n",
    "- Continuous: `f_classif`\n",
    "- Categorical, Nominal: `chi2` \n",
    "\n",
    "---\n",
    "\n",
    "(Explanation of ANOVA concepts, Chinese version)\n",
    "\n",
    "**【前提假設】**\n",
    "\n",
    "ANOVA test(變異數分析)的主要功能是用來**檢定多組相互獨立樣本的母體平均數是否具有顯著差異**(可以理解為檢定多組樣本間是否不同)；例如：亞洲各國家的成人平均身高是否相同。\n",
    "在進行檢驗前，我們希望確定每組獨立樣本的平均數的確能夠被拿來互相比較；因此各組樣本的母體除了需要符合常態分布外，還希望其資料離散分布的狀況能具有相似性，也就是說，`樣本的母體變異數必須具有同質性`。\n",
    "嚴格來說，進行變異數分析之前，我們會先進行樣本母體變異數的同質性檢定，若各樣本的母體變異數皆具有同質性，就可以使用變異數分析來進行之後的檢定。\n",
    "但我們這邊因為只用作變數間的比較及排序，所以跳過此一步驟。\n",
    "\n",
    "**【量化樣本間的差異：利用樣本變異程度】**\n",
    "- 組間變異(Between-group variation)：不同組樣本之間的資料變異程度\n",
    "- 組內變異(Within-group variation)：各組樣本本身資料間的變異程度\n",
    "\n",
    "得到樣本的變異資訊後，就可以利用組間變異與組內變異的相對大小來檢定不同組樣本之間母體平均數的差異。\n",
    "`組間差異越大`、`組內差異越小`的時候，表示這個變數在Y=0和Y=1的人身上的`數據長相差異越大(f-value越大)`。\n",
    "另一種角度說就是這個變數的值可以很好的區別Y=0和Y=1的人。(->how well this feature discriminates between two classes)\n",
    "\n",
    "**【針對變數類型採用不同方法】**\n",
    "- 連續數值型變數：f_classif（F檢定）\n",
    "- 離散且不連續、類別型變數：chi2（卡方檢定）\n",
    "\n",
    "\n",
    "[參考資料: F-value](https://datascience.stackexchange.com/questions/74465/how-to-understand-anova-f-for-feature-selection-in-python-sklearn-selectkbest-w)\n",
    " / [ANOVA test 易踩誤區](https://towardsdatascience.com/mistakes-in-applying-univariate-feature-selection-methods-34c43ce8b93d)\n",
    " / [ANOVA 變異數分析基本概念](https://yourgene.pixnet.net/blog/post/118650399-%E8%AE%8A%E7%95%B0%E6%95%B8%E5%88%86%E6%9E%90anova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_analysis(data, percentile, plot=True):\n",
    "    from sklearn.feature_selection import SelectPercentile, f_classif, chi2\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    def conduct_anova(data, ftype, plot, percentile):\n",
    "        \n",
    "        # data type inspector\n",
    "        if ftype == 'numerical':\n",
    "            f_list = list(set(data.columns[data.dtypes != 'object']) & set(data.columns[data.dtypes != 'bool']))\n",
    "            f_list.remove('y')\n",
    "        elif ftype == 'categorical':\n",
    "            f_list = list(set(data.columns[data.dtypes == 'object']) | set(data.columns[data.dtypes == 'bool']))\n",
    "            f_list.remove('y')\n",
    "        \n",
    "        X = data[f_list]\n",
    "        X = X.fillna(0)\n",
    "        y = data['y'] \n",
    "\n",
    "        # normalize\n",
    "        if ftype == 'numerical': \n",
    "            normalizer = Normalizer()\n",
    "            X = normalizer.fit_transform(X)\n",
    "            X = pd.DataFrame(X, columns = f_list)\n",
    "        \n",
    "        # turn str and bool to numerical values (for chi-test)\n",
    "        X = X.apply(lambda x: pd.factorize(x)[0] if x.dtype == 'bool' or x.dtype == 'object' else x)\n",
    "        f_cnts = X.shape[-1]\n",
    "        if f_cnts == 0:\n",
    "            return None    \n",
    "\n",
    "        if ftype == 'numerical':\n",
    "            selector = SelectPercentile(f_classif, percentile)\n",
    "        elif ftype == 'categorical':\n",
    "            selector = SelectPercentile(chi2, percentile)\n",
    "            \n",
    "        selector.fit(X, y)\n",
    "        keep_indices = selector.get_support(indices=True)\n",
    "        keep_features, score, p_value = np.array(X.columns[keep_indices]), selector.scores_[keep_indices], selector.pvalues_[keep_indices]\n",
    "        stats = pd.DataFrame(data = [keep_features, score, p_value]).T\n",
    "        stats.columns= [\"feature\", \"score (F or chi)\", \"p_value\"]\n",
    "        stats['dtype'] = [ftype] * stats.shape[0]\n",
    "        \n",
    "        if plot:\n",
    "            bar1 = score\n",
    "            bar2 = p_value\n",
    "            indx = np.arange(len(bar1))  \n",
    "\n",
    "            fig, (ax2, ax3) = plt.subplots(nrows=2, ncols=1, constrained_layout=True)\n",
    "            ax2.plot(indx, bar1)\n",
    "            ax3.plot(indx, bar2)\n",
    "            fig.suptitle('**************************  {}, {}  **************************'.format(d, ftype), fontsize=16)\n",
    "            ax2.set_title(\"score(F or chi)\")\n",
    "            ax3.set_title(\"p-value\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    for ftype in ['numerical', 'categorical']:\n",
    "        stats = conduct_anova(data, ftype, plot, percentile)\n",
    "        if stats is not None:\n",
    "            result = pd.concat([result, stats], ignore_index=True)\n",
    "            print(d, ftype, 'picked ---',stats.shape[0])\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_table = anova_analysis(data, percentile=50, plot=False)\n",
    "anova_features = anova_table[\"feature\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binner: get_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the features that passed filter I and II; these are features that are recommended to go on further analysis\n",
    "selected_features = list(set(anova_features) | set(informative_features))\n",
    "print(\"number of features after filter:\", len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply `Binner`, a self-defined class to find the optimized threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_threshold(data, selected_features, plot_binner=False):\n",
    "    from binner import NumericFeatureBinner\n",
    "    from binner import NominalFeatureBinner\n",
    "    \n",
    "    result = pd.DataFrame()\n",
    "    stats = {}\n",
    "    f_list = [f for f in selected_features if f in data.columns.tolist()]\n",
    "    numeric_f_list = list(set(data.columns[data.dtypes != 'object']) & set(data.columns[data.dtypes != 'bool']))\n",
    "    category_f_list = list(set(data.columns[data.dtypes == 'object']) | set(data.columns[data.dtypes == 'bool']))\n",
    "\n",
    "    for f in f_list:\n",
    "        X = pd.DataFrame(data[f]).reset_index(drop=True)\n",
    "        y = pd.DataFrame(data['y']).reset_index(drop=True)\n",
    "                \n",
    "        # Categorical features\n",
    "        if f in category_f_list:\n",
    "            binner = NominalFeatureBinner()\n",
    "            binner.fit(X, y, method='order', criteria='event_rate(%)')\n",
    "            \n",
    "            bin_table = binner.get_performance_df({f: [X, y]})\n",
    "            if 'NaN' in bin_table.columns.tolist():\n",
    "                bin_table = bin_table.drop(columns=['NaN'])\n",
    "                \n",
    "            bin_table = bin_table.T\n",
    "            bin_table['covered_bad_rate'] = bin_table['event_rate(%)'].astype(float)/100\n",
    "            max_bin = bin_table['covered_bad_rate'].idxmax()\n",
    "            \n",
    "            stats[f] = {\n",
    "                        'feature': f,\n",
    "                        'covered_count': bin_table.loc[max_bin, 'count'],\n",
    "                        'covered_bad': bin_table.loc[max_bin, 'event'],\n",
    "                        'covered_bad_rate': bin_table.loc[max_bin, 'covered_bad_rate'],\n",
    "                        'condition':bin_table.loc[max_bin, 'fields']\n",
    "                    }\n",
    "            if plot_binner:\n",
    "                binner.plot({f: [X, y]})\n",
    "        \n",
    "        # Numerical features\n",
    "        if f in numeric_f_list:\n",
    "            binner = NumericFeatureBinner()\n",
    "            binner.fit_auto_merge_bins(X, y, max_bins=20, method='tree', criteria='event_rate(%)')\n",
    "\n",
    "            bin_table = binner.get_performance_df({f: [X, y]})\n",
    "            if 'NaN' in bin_table.columns.tolist():\n",
    "                bin_table = bin_table.drop(columns=['NaN'])\n",
    "                \n",
    "            # filter again\n",
    "            # if (event rate in max bin/event rate in averge) is less than 1.5 => drop the feature\n",
    "            max_box = bin_table.loc['event_rate(%)'].astype(float).max()/100\n",
    "            min_box = bin_table.loc['event_rate(%)'].astype(float).min()/100\n",
    "            mean_box = bin_table.loc['event'].sum()/bin_table.loc['count'].sum()\n",
    "            bin_table = bin_table.T\n",
    "            bin_table['covered_bad_rate'] = bin_table['event_rate(%)'].astype(float)/100\n",
    "            \n",
    "            if (max_box/mean_box) >= 1.5:\n",
    "                max_bin = bin_table['covered_bad_rate'].idxmax()\n",
    "                condition = bin_table.loc[max_bin, 'upper_bound']\n",
    "                if condition == 'else':\n",
    "                    condition = '>=' + str(bin_table.loc[max_bin, 'min'])\n",
    "                else:\n",
    "                    condition = '<=' + str(bin_table.loc[max_bin, 'upper_bound'])\n",
    "                \n",
    "                name = f + '_bad'\n",
    "                stats[f] = {\n",
    "                            'feature': name,\n",
    "                            'covered_count': bin_table.loc[max_bin, 'count'],\n",
    "                            'covered_bad': bin_table.loc[max_bin, 'event'],\n",
    "                            'covered_bad_rate': bin_table.loc[max_bin, 'covered_bad_rate'],\n",
    "                            'condition': condition\n",
    "                        }\n",
    "                if plot_binner:\n",
    "                    binner.plot({f: [X, y]})\n",
    "                                \n",
    "            # if (event rate in min bin/event rate in average) is larger than 0.5 => drop the feature\n",
    "            if ((min_box/mean_box) <= 0.5) | (min_box == 0):\n",
    "                min_bin = bin_table['covered_bad_rate'].idxmin()\n",
    "                condition = bin_table.loc[min_bin, 'upper_bound']\n",
    "                if condition == 'else':\n",
    "                    condition = '>=' + str(bin_table.loc[min_bin, 'min'])\n",
    "                else:\n",
    "                    condition = '<=' + str(bin_table.loc[min_bin, 'upper_bound'])\n",
    "                \n",
    "                name = f + '_good'\n",
    "                stats[f] = {\n",
    "                            'feature': name,\n",
    "                            'covered_count': bin_table.loc[min_bin, 'count'],\n",
    "                            'covered_bad': bin_table.loc[min_bin, 'event'],\n",
    "                            'covered_bad_rate': bin_table.loc[min_bin, 'covered_bad_rate'],\n",
    "                            'condition': condition\n",
    "                        }\n",
    "\n",
    "                if plot_binner:\n",
    "                    binner.plot({f: [X, y]})\n",
    "\n",
    "    # turn stats into table\n",
    "    df = pd.DataFrame.from_dict(stats).T.reset_index(drop=True)\n",
    "    result = pd.concat([result, df], ignore_index=True)\n",
    "    result = result.sort_values(by=['covered_bad_rate','covered_bad'], ascending=False)\n",
    "    result = result.reset_index(drop=True)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_threshold(data, selected_features, plot_binner=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
